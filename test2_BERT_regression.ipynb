{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import modeling\n",
    "import tokenization\n",
    "\n",
    "import run_regressor\n",
    "\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_config_file = '/home/ubuntu/bert_models/uncased_L-12_H-768_A-12/bert_config.json'\n",
    "bert_config = modeling.BertConfig.from_json_file(bert_config_file)\n",
    "vocab_file = '/home/ubuntu/bert_models/uncased_L-12_H-768_A-12/vocab.txt'\n",
    "\n",
    "max_seq_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_REGRESSOR(object):\n",
    "    \n",
    "    def __init__(self,bert_config_file,max_seq_length,is_training):\n",
    "        self.bert_config_file = bert_config_file\n",
    "        self.bert_config = modeling.BertConfig.from_json_file(bert_config_file)\n",
    "        self.max_seq_length = max_seq_length\n",
    "        \n",
    "        ''' \n",
    "            Computation Graph\n",
    "        '''\n",
    "        input_ids = tf.placeholder(tf.int32,[None,max_seq_length],name='input_ids')\n",
    "        input_mask = tf.placeholder(tf.int32,[None,max_seq_length],name='input_mask')\n",
    "        segment_ids = tf.placeholder(tf.int32,[None,max_seq_length],name='segment_ids')\n",
    "        values = tf.placeholder(tf.float32,[None],name='values')\n",
    "        \n",
    "        model = modeling.BertModel(config=self.bert_config,\n",
    "                                   is_training=is_training,\n",
    "                                   input_ids=input_ids,\n",
    "                                   input_mask=input_mask,\n",
    "                                   token_type_ids=segment_ids,\n",
    "                                   use_one_hot_embeddings=False)\n",
    "        \n",
    "        output_layer = model.get_pooled_output()\n",
    "        \n",
    "        hidden_size = output_layer.shape[-1].value\n",
    "        \n",
    "        output_weights = tf.get_variable(\n",
    "          \"output_weights\", [1, hidden_size],\n",
    "          initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "\n",
    "        output_bias = tf.get_variable(\n",
    "          \"output_bias\", [1], initializer=tf.zeros_initializer())\n",
    "        \n",
    "        with tf.variable_scope(\"loss\"):\n",
    "            if is_training:\n",
    "                # I.e., 0.1 dropout\n",
    "                output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
    "\n",
    "            logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
    "            logits = tf.nn.bias_add(logits, output_bias)\n",
    "            logits = tf.squeeze(logits)\n",
    "\n",
    "            loss = tf.losses.mean_squared_error(values,logits)\n",
    "            self.loss = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_HANDLER(object):\n",
    "    \n",
    "    def __init__(self,processor,data_dir,vocab_file,max_seq_length):\n",
    "        self.processor = processor\n",
    "        self.tokenizer = tokenization.FullTokenizer(vocab_file=vocab_file,do_lower_case=True)\n",
    "        self.data_dir = data_dir\n",
    "        self.max_seq_length = max_seq_length\n",
    "    \n",
    "    def make_datasets(self,get_example_fn):\n",
    "        examples = get_example_fn(self.data_dir)\n",
    "        features = run_regressor.convert_examples_to_features(examples,\n",
    "                                                              max_seq_length=self.max_seq_length,\n",
    "                                                              tokenizer=self.tokenizer)\n",
    "        input_ids = []\n",
    "        input_mask = []\n",
    "        segment_ids = []\n",
    "        values = []\n",
    "        for feature in test_features:\n",
    "            input_ids.append(feature.input_ids)\n",
    "            input_mask.append(feature.input_mask)\n",
    "            segment_ids.append(feature.segment_ids)\n",
    "            values.append(feature.label_id)\n",
    "        \n",
    "        input_ids_tensor = tf.constant(input_ids)\n",
    "        input_mask_tensor = tf.constant(input_mask)\n",
    "        segment_ids_tensor = tf.constant(segment_ids)\n",
    "        values_tensor = tf.constant(values)\n",
    "        \n",
    "        datasets = tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(input_ids_tensor),\n",
    "                                        tf.data.Dataset.from_tensor_slices(input_mask_tensor),\n",
    "                                        tf.data.Dataset.from_tensor_slices(segment_ids_tensor),\n",
    "                                        tf.data.Dataset.from_tensor_slices(label_ids_tensor)))\n",
    "        return datasets\n",
    "        \n",
    "    \n",
    "    def fit():\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = run_regressor.ArdProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenization.FullTokenizer(vocab_file=vocab_file,do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_examples = processor.get_test_examples('/home/ubuntu/glue_data/ARD/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_features = run_regressor.convert_examples_to_features(test_examples,\n",
    "                                                            max_seq_length=max_seq_length,\n",
    "                                                            tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input_ids = []\n",
    "all_input_mask = []\n",
    "all_segment_ids = []\n",
    "all_label_ids = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in test_features:\n",
    "    all_input_ids.append(feature.input_ids)\n",
    "    all_input_mask.append(feature.input_mask)\n",
    "    all_segment_ids.append(feature.segment_ids)\n",
    "    all_label_ids.append(feature.label_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids=tf.constant(all_input_ids[:50])\n",
    "input_mask=tf.constant(all_input_mask[:50])\n",
    "segment_ids=tf.constant(all_segment_ids[:50])\n",
    "label_ids = tf.constant(all_label_ids[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(input_ids),\n",
    "                                tf.data.Dataset.from_tensor_slices(input_mask),\n",
    "                                tf.data.Dataset.from_tensor_slices(segment_ids),\n",
    "                                tf.data.Dataset.from_tensor_slices(label_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.output_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.output_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.estimator.Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " = datasets.batch(batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = modeling.BertModel(\n",
    "  config=bert_config,\n",
    "  is_training=True,\n",
    "  input_ids=input_ids,\n",
    "  input_mask=input_mask,\n",
    "  token_type_ids=segment_ids,\n",
    "  use_one_hot_embeddings=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 1\n",
    "is_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer = model.get_pooled_output()\n",
    "\n",
    "hidden_size = output_layer.shape[-1].value\n",
    "\n",
    "output_weights = tf.get_variable(\n",
    "  \"output_weights\", [1, hidden_size],\n",
    "  initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "\n",
    "output_bias = tf.get_variable(\n",
    "  \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
    "\n",
    "with tf.variable_scope(\"loss\"):\n",
    "    if is_training:\n",
    "        # I.e., 0.1 dropout\n",
    "        output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
    "\n",
    "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
    "    logits = tf.nn.bias_add(logits, output_bias)\n",
    "    #probabilities = tf.nn.softmax(logits, axis=-1)\n",
    "    #log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "    logits = tf.squeeze(logits)\n",
    "\n",
    "    loss = tf.losses.mean_squared_error(label_ids,logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "loss_ = loss.eval()\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
