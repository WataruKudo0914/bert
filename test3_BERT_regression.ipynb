{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert Regressor\n",
    "学習中にtrain_lossとeval_lossを同時に見れるようにする．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import modeling\n",
    "import tokenization\n",
    "import optimization\n",
    "import run_regressor\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing for Data\n",
    "give indice to reviewerID, asin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('/home/ubuntu/glue_data/ARD/train.tsv','\\t')\n",
    "dev_df = pd.read_csv('/home/ubuntu/glue_data/ARD/dev.tsv','\\t')\n",
    "test_df = pd.read_csv('/home/ubuntu/glue_data/ARD/test.tsv','\\t')\n",
    "all_df = train_df.append(dev_df).append(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(np.hstack((all_df.asin,all_df.reviewerID)))\n",
    "def add_indice_cols(df):\n",
    "    df_ = df.copy()\n",
    "    df_['user_id'] = label_encoder.transform(df_['reviewerID'])\n",
    "    df_['item_id'] = label_encoder.transform(df_['asin'])\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = add_indice_cols(train_df)\n",
    "dev_df = add_indice_cols(dev_df)\n",
    "test_df = add_indice_cols(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, dev_df = train_test_split(train_df.append(dev_df), test_size=0.33, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('/home/ubuntu/glue_data/ARD/train.tsv',sep='\\t',index=None)\n",
    "dev_df.to_csv('/home/ubuntu/glue_data/ARD/dev.tsv',sep='\\t',index=None)\n",
    "test_df.to_csv('/home/ubuntu/glue_data/ARD/test.tsv',sep='\\t',index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_config_file = '/home/ubuntu/bert_models/uncased_L-12_H-768_A-12/bert_config.json'\n",
    "bert_config = modeling.BertConfig.from_json_file(bert_config_file)\n",
    "vocab_file = '/home/ubuntu/bert_models/uncased_L-12_H-768_A-12/vocab.txt'\n",
    "max_seq_length = 128\n",
    "train_batch_size = 32\n",
    "eval_batch_size = 32\n",
    "predict_batch_size = 32\n",
    "num_train_epochs=5.0\n",
    "warmup_proportion = 0.1\n",
    "\n",
    "''' tpu '''\n",
    "save_checkpoints_steps = 1000\n",
    "iterations_per_loop = 1000\n",
    "\n",
    "DATA_DIR = '/home/ubuntu/glue_data/ARD/'\n",
    "BERT_BASE_DIR = '/home/ubuntu/bert_models/uncased_L-12_H-768_A-12'\n",
    "INIT_CHECKPOINT = '/home/ubuntu/bert_models/uncased_L-12_H-768_A-12/bert_model.ckpt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using only textual information\n",
    "def create_model(bert_config, is_training, input_ids, input_mask, segment_ids,values):\n",
    "    \"\"\"Creates a regression model.\"\"\"\n",
    "    model = modeling.BertModel(\n",
    "      config=bert_config,\n",
    "      is_training=is_training,\n",
    "      input_ids=input_ids,\n",
    "      input_mask=input_mask,\n",
    "      token_type_ids=segment_ids)\n",
    "\n",
    "    output_layer = model.get_pooled_output()\n",
    "\n",
    "    hidden_size = output_layer.shape[-1].value\n",
    "\n",
    "    output_weights = tf.get_variable(\n",
    "      \"output_weights\", [1, hidden_size],\n",
    "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "\n",
    "    output_bias = tf.get_variable(\n",
    "      \"output_bias\", [1], initializer=tf.zeros_initializer())\n",
    "\n",
    "    with tf.variable_scope(\"loss\"):\n",
    "        if is_training:\n",
    "          # I.e., 0.1 dropout\n",
    "          output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
    "\n",
    "        logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
    "        logits = tf.nn.bias_add(logits, output_bias)\n",
    "        logits = tf.squeeze(logits)\n",
    "        loss = tf.losses.mean_squared_error(values,logits)\n",
    "\n",
    "    return (loss, logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "レビュワー・商品のコンテクストを取り入れる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distribution(df):\n",
    "    user_cnt_dist = df.groupby(['user_id','overall'])['helpful_rate'].count().unstack(1,fill_value=0)\n",
    "    item_cnt_dist = df.groupby(['item_id','overall'])['helpful_rate'].count().unstack(1,fill_value=0)\n",
    "    all_cnt_dist = user_cnt_dist.append(item_cnt_dist).sort_index()\n",
    "    all_dist = all_cnt_dist.apply(lambda x:x/sum(x),1)\n",
    "    return all_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using textual and user-item contextual information\n",
    "def create_model2(bert_config, is_training, input_ids, input_mask, segment_ids,values,user_id,item_id,overall):\n",
    "    \"\"\"Creates a regression model.\"\"\"\n",
    "    # text information\n",
    "    model = modeling.BertModel(\n",
    "      config=bert_config,\n",
    "      is_training=is_training,\n",
    "      input_ids=input_ids,\n",
    "      input_mask=input_mask,\n",
    "      token_type_ids=segment_ids)\n",
    "\n",
    "    output_layer = model.get_pooled_output()\n",
    "    \n",
    "    \n",
    "    # User and Item and this rating information\n",
    "    user_item_emb = tf.constant(get_distribution(all_df).values,dtype=tf.float32)\n",
    "    user_dist = tf.nn.embedding_lookup(user_item_emb,user_id)\n",
    "    item_dist = tf.nn.embedding_lookup(user_item_emb,item_id)\n",
    "    rating_onehot = tf.one_hot(overall-1,depth=5,dtype=tf.float32)\n",
    "    historical_ = tf.concat([user_dist,item_dist,rating_onehot],1)\n",
    "    historical = tf.layers.dense(historical_,historical_.shape[-1].value)\n",
    "    \n",
    "    # concat\n",
    "    bert_historical = tf.concat([output_layer,historical],1)\n",
    "    \n",
    "    hidden_size = bert_historical.shape[-1].value\n",
    "    \n",
    "    output_weights = tf.get_variable(\n",
    "      \"output_weights\", [1, hidden_size],\n",
    "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "\n",
    "    output_bias = tf.get_variable(\n",
    "      \"output_bias\", [1], initializer=tf.zeros_initializer())\n",
    "\n",
    "    with tf.variable_scope(\"loss\"):\n",
    "        if is_training:\n",
    "          # I.e., 0.1 dropout\n",
    "          output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
    "\n",
    "        logits = tf.matmul(bert_historical, output_weights, transpose_b=True)\n",
    "        logits = tf.nn.bias_add(logits, output_bias)\n",
    "        logits = tf.squeeze(logits)\n",
    "        loss = tf.losses.mean_squared_error(values,logits)\n",
    "\n",
    "    return (loss, logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn_builder(bert_config, init_checkpoint, learning_rate,\n",
    "                     num_train_steps, num_warmup_steps, use_tpu=False):\n",
    "  \"\"\"Returns `model_fn` closure\"\"\"\n",
    "\n",
    "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
    "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
    "    input_ids = features[\"input_ids\"]\n",
    "    input_mask = features[\"input_mask\"]\n",
    "    segment_ids = features[\"segment_ids\"]\n",
    "    values = features[\"values\"]\n",
    "    user_id = features[\"user_id\"]\n",
    "    item_id = features[\"item_id\"]\n",
    "    overall = features[\"overall\"]\n",
    "\n",
    "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "#     (total_loss, logits) = create_model2(\n",
    "#         bert_config, is_training, input_ids, input_mask, segment_ids, values,user_id,item_id,overall)\n",
    "    (total_loss, logits) = create_model(\n",
    "        bert_config, is_training, input_ids, input_mask, segment_ids, values)\n",
    "\n",
    "    tvars = tf.trainable_variables()\n",
    "    initialized_variable_names = {}\n",
    "    scaffold_fn = None\n",
    "    if init_checkpoint:\n",
    "        (assignment_map, initialized_variable_names\n",
    "              ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n",
    "        if use_tpu:\n",
    "\n",
    "            def tpu_scaffold():\n",
    "                tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
    "                return tf.train.Scaffold()\n",
    "\n",
    "            scaffold_fn = tpu_scaffold\n",
    "        else:\n",
    "            tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
    "\n",
    "    output_spec = None\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "\n",
    "        train_op = optimization.create_optimizer(total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n",
    "\n",
    "        output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
    "                              mode=mode,\n",
    "                              loss=total_loss,\n",
    "                              train_op=train_op,\n",
    "                              scaffold_fn=scaffold_fn)\n",
    "    elif mode == tf.estimator.ModeKeys.EVAL:\n",
    "\n",
    "        def metric_fn(total_loss, logits):\n",
    "            return {\n",
    "                \"eval_loss\": tf.metrics.mean_squared_error(labels=values,predictions=logits),\n",
    "            }\n",
    "\n",
    "        eval_metrics = (metric_fn, [total_loss, logits])\n",
    "        output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
    "          mode=mode,\n",
    "          loss=total_loss,\n",
    "          eval_metrics=eval_metrics,\n",
    "          scaffold_fn=scaffold_fn)\n",
    "    else:\n",
    "        output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
    "            mode=mode, predictions=logits, scaffold_fn=scaffold_fn)\n",
    "    return output_spec\n",
    "\n",
    "  return model_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn_builder(features, seq_length, is_training, drop_remainder):\n",
    "    \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n",
    "\n",
    "    all_input_ids = []\n",
    "    all_input_mask = []\n",
    "    all_segment_ids = []\n",
    "    all_label_ids = []\n",
    "    all_user_ids = []\n",
    "    all_item_ids = []\n",
    "    all_overalls = []\n",
    "\n",
    "    for feature in features:\n",
    "        all_input_ids.append(feature.input_ids)\n",
    "        all_input_mask.append(feature.input_mask)\n",
    "        all_segment_ids.append(feature.segment_ids)\n",
    "        all_label_ids.append(feature.label_id)\n",
    "        all_user_ids.append(feature.user_id)\n",
    "        all_item_ids.append(feature.item_id)\n",
    "        all_overalls.append(feature.overall)\n",
    "  \n",
    "    def input_fn(params):\n",
    "        \"\"\"The actual input function.\"\"\"\n",
    "        batch_size = params[\"batch_size\"]\n",
    "\n",
    "        num_examples = len(features)\n",
    "\n",
    "        # This is for demo purposes and does NOT scale to large data sets. We do\n",
    "        # not use Dataset.from_generator() because that uses tf.py_func which is\n",
    "        # not TPU compatible. The right way to load data is with TFRecordReader.\n",
    "        d = tf.data.Dataset.from_tensor_slices({\n",
    "            \"input_ids\":\n",
    "                tf.constant(\n",
    "                    all_input_ids, shape=[num_examples, seq_length],\n",
    "                    dtype=tf.int32),\n",
    "            \"input_mask\":\n",
    "                tf.constant(\n",
    "                    all_input_mask,\n",
    "                    shape=[num_examples, seq_length],\n",
    "                    dtype=tf.int32),\n",
    "            \"segment_ids\":\n",
    "                tf.constant(\n",
    "                    all_segment_ids,\n",
    "                    shape=[num_examples, seq_length],\n",
    "                    dtype=tf.int32),\n",
    "            \"values\":\n",
    "                tf.constant(all_label_ids, shape=[num_examples], dtype=tf.float32),\n",
    "            \"user_id\":\n",
    "                tf.constant(all_user_ids,shape=[num_examples],dtype=tf.int32),\n",
    "            \"item_id\":\n",
    "                tf.constant(all_item_ids,shape=[num_examples],dtype=tf.int32),\n",
    "            \"overall\":\n",
    "                tf.constant(all_overalls,shape=[num_examples],dtype=tf.int32),\n",
    "        })\n",
    "\n",
    "        if is_training:\n",
    "            d = d.repeat()\n",
    "            d = d.shuffle(buffer_size=100)\n",
    "\n",
    "        d = d.batch(batch_size=batch_size, drop_remainder=drop_remainder)\n",
    "        return d\n",
    "\n",
    "    return input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = '/home/ubuntu/tmp/bert_TrEv/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = run_regressor.ArdProcessor()\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file=vocab_file,do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = processor.get_train_examples(DATA_DIR)\n",
    "num_train_steps = int(len(train_examples) / train_batch_size * num_train_epochs)\n",
    "num_warmup_steps = int(num_train_steps * warmup_proportion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tpu configuration\n",
    "tpu_cluster_resolver = None\n",
    "is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n",
    "run_config = tf.contrib.tpu.RunConfig(\n",
    "  cluster=tpu_cluster_resolver,\n",
    "  master=None,\n",
    "  model_dir=OUTPUT_DIR,\n",
    "  save_checkpoints_steps=save_checkpoints_steps,\n",
    "  tpu_config=tf.contrib.tpu.TPUConfig(\n",
    "      iterations_per_loop=iterations_per_loop,\n",
    "      num_shards=None,\n",
    "      per_host_input_for_training=is_per_host))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = model_fn_builder(bert_config=bert_config,\n",
    "                            init_checkpoint=INIT_CHECKPOINT,\n",
    "                            learning_rate=2e-5,\n",
    "                            num_train_steps=num_train_steps,\n",
    "                            num_warmup_steps=num_warmup_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7f20d8a89f28>) includes params argument, but params are not passed to Estimator.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/home/ubuntu/tmp/bert_TrEv/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f20e5420048>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=1000, num_shards=None, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None), '_cluster': None}\n",
      "INFO:tensorflow:_TPUContext: eval_on_tpu True\n",
      "WARNING:tensorflow:eval_on_tpu ignored because use_tpu is False.\n"
     ]
    }
   ],
   "source": [
    "estimator = tf.contrib.tpu.TPUEstimator(\n",
    "    use_tpu=False,\n",
    "    model_fn=model_fn,\n",
    "    config=run_config,\n",
    "    train_batch_size=train_batch_size,\n",
    "    eval_batch_size=eval_batch_size,\n",
    "    predict_batch_size=predict_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "steps_per_eval = 100 #training_step何回につき評価を行うか"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 33500\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: train-0\n",
      "INFO:tensorflow:tokens: [CLS] i have a nano ipod , while my sons have the video ipod ##s . i drop mine a lot , cutting the lawn , etc , and nano ##s are just about ind ##est ##ru ##ct ##ible . i keep all my music on the computer , and about 10 hours worth on the nano . i love it . i searched carefully , first reading every review of a small stereo unit to play the ipod ##s . next , i went to several stores and tested them all , with my ipod , playing bob dylan music . i download his concerts , so the sound quality is not always the best ; which is a great way to test the various units [SEP]\n",
      "INFO:tensorflow:input_ids: 101 1045 2031 1037 28991 26322 1010 2096 2026 4124 2031 1996 2678 26322 2015 1012 1045 4530 3067 1037 2843 1010 6276 1996 10168 1010 4385 1010 1998 28991 2015 2024 2074 2055 27427 4355 6820 6593 7028 1012 1045 2562 2035 2026 2189 2006 1996 3274 1010 1998 2055 2184 2847 4276 2006 1996 28991 1012 1045 2293 2009 1012 1045 9022 5362 1010 2034 3752 2296 3319 1997 1037 2235 12991 3131 2000 2377 1996 26322 2015 1012 2279 1010 1045 2253 2000 2195 5324 1998 7718 2068 2035 1010 2007 2026 26322 1010 2652 3960 7758 2189 1012 1045 8816 2010 6759 1010 2061 1996 2614 3737 2003 2025 2467 1996 2190 1025 2029 2003 1037 2307 2126 2000 3231 1996 2536 3197 102\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: train-1\n",
      "INFO:tensorflow:tokens: [CLS] as a former professional photographer ( back in the old days ) i did not rush out to buy a digital point - and - shoot when they first came out , as the quality was poor compared to film . also , they become obsolete very quickly . my first dig ##ica ##m was a powers ##hot a ##51 ##0 - a solid little camera - and i took several award - winning photos with it . i ' ve used sony and pan ##as ##onic dig ##ica ##ms , and have looked at all the other common makes . at this price point , i haven ' t seen anything that i like better than the 570 ##is for features and ease of use [SEP]\n",
      "INFO:tensorflow:input_ids: 101 2004 1037 2280 2658 8088 1006 2067 1999 1996 2214 2420 1007 1045 2106 2025 5481 2041 2000 4965 1037 3617 2391 1011 1998 1011 5607 2043 2027 2034 2234 2041 1010 2004 1996 3737 2001 3532 4102 2000 2143 1012 2036 1010 2027 2468 15832 2200 2855 1012 2026 2034 10667 5555 2213 2001 1037 4204 12326 1037 22203 2692 1011 1037 5024 2210 4950 1011 1998 1045 2165 2195 2400 1011 3045 7760 2007 2009 1012 1045 1005 2310 2109 8412 1998 6090 3022 12356 10667 5555 5244 1010 1998 2031 2246 2012 2035 1996 2060 2691 3084 1012 2012 2023 3976 2391 1010 1045 4033 1005 1056 2464 2505 2008 1045 2066 2488 2084 1996 24902 2483 2005 2838 1998 7496 1997 2224 102\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: train-2\n",
      "INFO:tensorflow:tokens: [CLS] this isn ' t really a review , they ' re just batteries after all , but i did want to alleviate something that had confused me . if you look at the picture it ' s hard to tell that these are lithium . from what i understand all cr ##12 ##3 batteries are . what threw me off was the packaging in the product picture , which is different than en ##er ##gi ##zer ' s current packaging for lithium batteries . when the batteries arrived , however they were the expected type and in the newer packaging . [SEP]\n",
      "INFO:tensorflow:input_ids: 101 2023 3475 1005 1056 2428 1037 3319 1010 2027 1005 2128 2074 10274 2044 2035 1010 2021 1045 2106 2215 2000 24251 2242 2008 2018 5457 2033 1012 2065 2017 2298 2012 1996 3861 2009 1005 1055 2524 2000 2425 2008 2122 2024 22157 1012 2013 2054 1045 3305 2035 13675 12521 2509 10274 2024 1012 2054 4711 2033 2125 2001 1996 14793 1999 1996 4031 3861 1010 2029 2003 2367 2084 4372 2121 5856 6290 1005 1055 2783 14793 2005 22157 10274 1012 2043 1996 10274 3369 1010 2174 2027 2020 1996 3517 2828 1998 1999 1996 10947 14793 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: train-3\n",
      "INFO:tensorflow:tokens: [CLS] i just got this yesterday from amazon . i got the orange model because it just looks so st ##yl ##ish ( in picture and , indeed , in person ) . this is to be my backup compact camera , when i don ' t have my beloved fuji ##film fine ##pi ##x f ##30 6 ##mp camera with me . ( compared to the f ##30 , the v ##10 is taller but thinner and shorter . they weigh about the same , i think . ) here ' re my initial impressions : picture quality : very good outdoors during the day , and average / above - average indoors . fuji ##film ( aka fuji ) has produced a line of compact [SEP]\n",
      "INFO:tensorflow:input_ids: 101 1045 2074 2288 2023 7483 2013 9733 1012 1045 2288 1996 4589 2944 2138 2009 2074 3504 2061 2358 8516 4509 1006 1999 3861 1998 1010 5262 1010 1999 2711 1007 1012 2023 2003 2000 2022 2026 10200 9233 4950 1010 2043 1045 2123 1005 1056 2031 2026 11419 20933 23665 2986 8197 2595 1042 14142 1020 8737 4950 2007 2033 1012 1006 4102 2000 1996 1042 14142 1010 1996 1058 10790 2003 12283 2021 23082 1998 7820 1012 2027 17042 2055 1996 2168 1010 1045 2228 1012 1007 2182 1005 2128 2026 3988 19221 1024 3861 3737 1024 2200 2204 19350 2076 1996 2154 1010 1998 2779 1013 2682 1011 2779 24274 1012 20933 23665 1006 9875 20933 1007 2038 2550 1037 2240 1997 9233 102\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: train-4\n",
      "INFO:tensorflow:tokens: [CLS] just bought the bel 95 ##5 to replace my bel 980 . from what i have read the 95 ##5 gives all the advanced detection of more expensive bel products , with some lacking features that you may not miss . compared to my old 980 , the 95 ##5 has much , much less false alert ##s and seems to pick up k and ka radar much sooner . it is also very easy to program , but to be honest the factory settings are great and not much needs to be changed . the new auto - quiet feature is very nice . with this you get the alert at the sound level you picked , then it automatically goes softer after a few [SEP]\n",
      "INFO:tensorflow:input_ids: 101 2074 4149 1996 19337 5345 2629 2000 5672 2026 19337 25195 1012 2013 2054 1045 2031 3191 1996 5345 2629 3957 2035 1996 3935 10788 1997 2062 6450 19337 3688 1010 2007 2070 11158 2838 2008 2017 2089 2025 3335 1012 4102 2000 2026 2214 25195 1010 1996 5345 2629 2038 2172 1010 2172 2625 6270 9499 2015 1998 3849 2000 4060 2039 1047 1998 10556 7217 2172 10076 1012 2009 2003 2036 2200 3733 2000 2565 1010 2021 2000 2022 7481 1996 4713 10906 2024 2307 1998 2025 2172 3791 2000 2022 2904 1012 1996 2047 8285 1011 4251 3444 2003 2200 3835 1012 2007 2023 2017 2131 1996 9499 2012 1996 2614 2504 2017 3856 1010 2059 2009 8073 3632 19013 2044 1037 2261 102\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:Writing example 10000 of 33500\n",
      "INFO:tensorflow:Writing example 20000 of 33500\n",
      "INFO:tensorflow:Writing example 30000 of 33500\n"
     ]
    }
   ],
   "source": [
    "train_examples = processor.get_train_examples(DATA_DIR)\n",
    "\n",
    "train_features = run_regressor.convert_examples_to_features(train_examples,max_seq_length=max_seq_length,tokenizer=tokenizer)\n",
    "\n",
    "train_input_fn = input_fn_builder(train_features,max_seq_length,is_training=True,drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 16500\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: dev-0\n",
      "INFO:tensorflow:tokens: [CLS] as another person coming from the official un - lit leather case ( due to the re ##boot ##ing issue that caused it to be pulled from amazon ' s store ) i wanted something similar , without a light , that didn ' t use the battery bay clips causing all the issues . i finally stumbled upon this case and decided to take a chance based on other reviews . i consider myself \" spoiled \" by the simplicity and design of the old official leather case , so weed ##ing through the multitude of low - quality or over - the - top cases available , i was glad to see on manufacturer focusing on a simple yet well - built case . [SEP]\n",
      "INFO:tensorflow:input_ids: 101 2004 2178 2711 2746 2013 1996 2880 4895 1011 5507 5898 2553 1006 2349 2000 1996 2128 27927 2075 3277 2008 3303 2009 2000 2022 2766 2013 9733 1005 1055 3573 1007 1045 2359 2242 2714 1010 2302 1037 2422 1010 2008 2134 1005 1056 2224 1996 6046 3016 15281 4786 2035 1996 3314 1012 1045 2633 9845 2588 2023 2553 1998 2787 2000 2202 1037 3382 2241 2006 2060 4391 1012 1045 5136 2870 1000 19582 1000 2011 1996 17839 1998 2640 1997 1996 2214 2880 5898 2553 1010 2061 17901 2075 2083 1996 20889 1997 2659 1011 3737 2030 2058 1011 1996 1011 2327 3572 2800 1010 1045 2001 5580 2000 2156 2006 7751 7995 2006 1037 3722 2664 2092 1011 2328 2553 1012 102\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: dev-1\n",
      "INFO:tensorflow:tokens: [CLS] . . . this is it . sync ' ing with multiple computers or the web to back up your important data is fine - - when you ' re next to a computer to perform the sync . but if your batteries suddenly die and and you have to completely restore , murphy ' s law will have it that you ' re not next to your computer when that happens . the backup module reduces that possibility by neatly sitting in the vis ##or ' s spring ##board slot . the 1 - button backup and restore makes it easy to backup and restore . it is a great complement to your normal backup routines with hot ##sy ##nc ' ing . [SEP]\n",
      "INFO:tensorflow:input_ids: 101 1012 1012 1012 2023 2003 2009 1012 26351 1005 13749 2007 3674 7588 2030 1996 4773 2000 2067 2039 2115 2590 2951 2003 2986 1011 1011 2043 2017 1005 2128 2279 2000 1037 3274 2000 4685 1996 26351 1012 2021 2065 2115 10274 3402 3280 1998 1998 2017 2031 2000 3294 9239 1010 7104 1005 1055 2375 2097 2031 2009 2008 2017 1005 2128 2025 2279 2000 2115 3274 2043 2008 6433 1012 1996 10200 11336 13416 2008 6061 2011 15981 3564 1999 1996 25292 2953 1005 1055 3500 6277 10453 1012 1996 1015 1011 6462 10200 1998 9239 3084 2009 3733 2000 10200 1998 9239 1012 2009 2003 1037 2307 13711 2000 2115 3671 10200 23964 2007 2980 6508 12273 1005 13749 1012 102 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: dev-2\n",
      "INFO:tensorflow:tokens: [CLS] i ordered this along with my ga ##rmin nu ##vi 76 ##5 ##t and it never worked . thankfully , amazon did it ' s usual superb job of replacing it with just a few mouse clicks . when the replacement arrived it was of a much simpler design and worked just fine . detail : the original version i got had a slot in the back of the plug - in unit into which various plug ##s could be inserted to adapt it for the appropriate country / voltage / etc . my guess is that the connection between the adapt ##er and the unit were not good and so it just didn ' t work . the replacement unit which i received was made [SEP]\n",
      "INFO:tensorflow:input_ids: 101 1045 3641 2023 2247 2007 2026 11721 27512 16371 5737 6146 2629 2102 1998 2009 2196 2499 1012 16047 1010 9733 2106 2009 1005 1055 5156 21688 3105 1997 6419 2009 2007 2074 1037 2261 8000 29225 1012 2043 1996 6110 3369 2009 2001 1997 1037 2172 16325 2640 1998 2499 2074 2986 1012 6987 1024 1996 2434 2544 1045 2288 2018 1037 10453 1999 1996 2067 1997 1996 13354 1011 1999 3131 2046 2029 2536 13354 2015 2071 2022 12889 2000 15581 2009 2005 1996 6413 2406 1013 10004 1013 4385 1012 2026 3984 2003 2008 1996 4434 2090 1996 15581 2121 1998 1996 3131 2020 2025 2204 1998 2061 2009 2074 2134 1005 1056 2147 1012 1996 6110 3131 2029 1045 2363 2001 2081 102\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: dev-3\n",
      "INFO:tensorflow:tokens: [CLS] i would love to give this device a good review . when we got it , it was well - packaged , well - constructed , and seemed to work right away out of the box . all in all , it seemed to be a good design . unfortunately , it turns out that the good hardware design is cancelled out by g ##lit ##ches that make this device useless to us . the second screen ( the one plug ##ged into this device ) repeatedly locks up , freezing as if put on pause . sometimes even the mouse pointer is visible on the screen but doesn ' t respond to mouse movement ( although the main screen continues to work fine ) . [SEP]\n",
      "INFO:tensorflow:input_ids: 101 1045 2052 2293 2000 2507 2023 5080 1037 2204 3319 1012 2043 2057 2288 2009 1010 2009 2001 2092 1011 21972 1010 2092 1011 3833 1010 1998 2790 2000 2147 2157 2185 2041 1997 1996 3482 1012 2035 1999 2035 1010 2009 2790 2000 2022 1037 2204 2640 1012 6854 1010 2009 4332 2041 2008 1996 2204 8051 2640 2003 8014 2041 2011 1043 15909 8376 2008 2191 2023 5080 11809 2000 2149 1012 1996 2117 3898 1006 1996 2028 13354 5999 2046 2023 5080 1007 8385 11223 2039 1010 12809 2004 2065 2404 2006 8724 1012 2823 2130 1996 8000 20884 2003 5710 2006 1996 3898 2021 2987 1005 1056 6869 2000 8000 2929 1006 2348 1996 2364 3898 4247 2000 2147 2986 1007 1012 102\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: dev-4\n",
      "INFO:tensorflow:tokens: [CLS] fuji fine ##pi ##x s ##45 ##00 - 14 mp , 30 ##x zoom , 3 \" lcd screen , 4 aa bat ##eries , camera strap , lens cover with hanging loop and cables to hook to the tv in the box . it weighs almost a pound , but with the strap - it ' s fine . our new fuji is an awesome , well - priced higher - end camera with lost of features . only had it a week , but it has point & shoot features for those who are fairly new to digital , plus many high - end features for someone who is really an advanced photographer . i love this camera ! ! our nik ##on 8 [SEP]\n",
      "INFO:tensorflow:input_ids: 101 20933 2986 8197 2595 1055 19961 8889 1011 2403 6131 1010 2382 2595 24095 1010 1017 1000 27662 3898 1010 1018 9779 7151 28077 1010 4950 16195 1010 10014 3104 2007 5689 7077 1998 15196 2000 8103 2000 1996 2694 1999 1996 3482 1012 2009 21094 2471 1037 9044 1010 2021 2007 1996 16195 1011 2009 1005 1055 2986 1012 2256 2047 20933 2003 2019 12476 1010 2092 1011 21125 3020 1011 2203 4950 2007 2439 1997 2838 1012 2069 2018 2009 1037 2733 1010 2021 2009 2038 2391 1004 5607 2838 2005 2216 2040 2024 7199 2047 2000 3617 1010 4606 2116 2152 1011 2203 2838 2005 2619 2040 2003 2428 2019 3935 8088 1012 1045 2293 2023 4950 999 999 2256 23205 2239 1022 102\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:Writing example 10000 of 16500\n"
     ]
    }
   ],
   "source": [
    "eval_examples = processor.get_dev_examples(DATA_DIR)\n",
    "eval_features = run_regressor.convert_examples_to_features(eval_examples,max_seq_length=max_seq_length,tokenizer=tokenizer)\n",
    "# This tells the estimator to run through the entire set.\n",
    "eval_steps = None\n",
    "eval_input_fn = input_fn_builder(eval_features,max_seq_length,is_training=False,drop_remainder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Running train on CPU\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/tmp/bert_TrEv/model.ckpt-0\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /home/ubuntu/tmp/bert_TrEv/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 100 into /home/ubuntu/tmp/bert_TrEv/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.03993911.\n",
      "INFO:tensorflow:training_loop marked as finished\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Running eval on CPU\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-31-08:53:24\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/tmp/bert_TrEv/model.ckpt-100\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-31-08:58:41\n",
      "INFO:tensorflow:Saving dict for global step 100: eval_loss = 0.045444958, global_step = 100, loss = 0.04544542\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 100: /home/ubuntu/tmp/bert_TrEv/model.ckpt-100\n",
      "INFO:tensorflow:evaluation_loop marked as finished\n",
      "ステップ：100\n",
      "{'eval_loss': 0.045444958, 'loss': 0.04544542, 'global_step': 100}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Running train on CPU\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/tmp/bert_TrEv/model.ckpt-100\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 100 into /home/ubuntu/tmp/bert_TrEv/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 200 into /home/ubuntu/tmp/bert_TrEv/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.04796927.\n",
      "INFO:tensorflow:training_loop marked as finished\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Running eval on CPU\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-31-09:03:14\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/tmp/bert_TrEv/model.ckpt-200\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-31-09:08:32\n",
      "INFO:tensorflow:Saving dict for global step 200: eval_loss = 0.03674888, global_step = 200, loss = 0.036746137\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 200: /home/ubuntu/tmp/bert_TrEv/model.ckpt-200\n",
      "INFO:tensorflow:evaluation_loop marked as finished\n",
      "ステップ：200\n",
      "{'eval_loss': 0.03674888, 'loss': 0.036746137, 'global_step': 200}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Running train on CPU\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/tmp/bert_TrEv/model.ckpt-200\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 200 into /home/ubuntu/tmp/bert_TrEv/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 300 into /home/ubuntu/tmp/bert_TrEv/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.028724585.\n",
      "INFO:tensorflow:training_loop marked as finished\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Running eval on CPU\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-31-09:13:03\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/tmp/bert_TrEv/model.ckpt-300\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-31-09:18:21\n",
      "INFO:tensorflow:Saving dict for global step 300: eval_loss = 0.037033662, global_step = 300, loss = 0.037027743\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 300: /home/ubuntu/tmp/bert_TrEv/model.ckpt-300\n",
      "INFO:tensorflow:evaluation_loop marked as finished\n",
      "ステップ：300\n",
      "{'eval_loss': 0.037033662, 'loss': 0.037027743, 'global_step': 300}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Running train on CPU\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/tmp/bert_TrEv/model.ckpt-300\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 300 into /home/ubuntu/tmp/bert_TrEv/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 400 into /home/ubuntu/tmp/bert_TrEv/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.009519731.\n",
      "INFO:tensorflow:training_loop marked as finished\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Running eval on CPU\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-31-09:22:52\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/tmp/bert_TrEv/model.ckpt-400\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-31-09:28:09\n",
      "INFO:tensorflow:Saving dict for global step 400: eval_loss = 0.039462194, global_step = 400, loss = 0.03944792\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 400: /home/ubuntu/tmp/bert_TrEv/model.ckpt-400\n",
      "INFO:tensorflow:evaluation_loop marked as finished\n",
      "ステップ：400\n",
      "{'eval_loss': 0.039462194, 'loss': 0.03944792, 'global_step': 400}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Running train on CPU\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/tmp/bert_TrEv/model.ckpt-400\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 400 into /home/ubuntu/tmp/bert_TrEv/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 500 into /home/ubuntu/tmp/bert_TrEv/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.023045301.\n",
      "INFO:tensorflow:training_loop marked as finished\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Running eval on CPU\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-31-09:32:40\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/tmp/bert_TrEv/model.ckpt-500\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-31-09:37:58\n",
      "INFO:tensorflow:Saving dict for global step 500: eval_loss = 0.03548828, global_step = 500, loss = 0.035483338\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 500: /home/ubuntu/tmp/bert_TrEv/model.ckpt-500\n",
      "INFO:tensorflow:evaluation_loop marked as finished\n",
      "ステップ：500\n",
      "{'eval_loss': 0.03548828, 'loss': 0.035483338, 'global_step': 500}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Running train on CPU\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/tmp/bert_TrEv/model.ckpt-500\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 500 into /home/ubuntu/tmp/bert_TrEv/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 600 into /home/ubuntu/tmp/bert_TrEv/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.020410642.\n",
      "INFO:tensorflow:training_loop marked as finished\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Running eval on CPU\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-31-09:42:29\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/tmp/bert_TrEv/model.ckpt-600\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-31-09:47:47\n",
      "INFO:tensorflow:Saving dict for global step 600: eval_loss = 0.04640263, global_step = 600, loss = 0.046394445\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 600: /home/ubuntu/tmp/bert_TrEv/model.ckpt-600\n",
      "INFO:tensorflow:evaluation_loop marked as finished\n",
      "ステップ：600\n",
      "{'eval_loss': 0.04640263, 'loss': 0.046394445, 'global_step': 600}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Running train on CPU\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/tmp/bert_TrEv/model.ckpt-600\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 600 into /home/ubuntu/tmp/bert_TrEv/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 700 into /home/ubuntu/tmp/bert_TrEv/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.011527933.\n",
      "INFO:tensorflow:training_loop marked as finished\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Running eval on CPU\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-31-09:52:17\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/tmp/bert_TrEv/model.ckpt-700\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-31-09:57:36\n",
      "INFO:tensorflow:Saving dict for global step 700: eval_loss = 0.04941989, global_step = 700, loss = 0.049409322\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 700: /home/ubuntu/tmp/bert_TrEv/model.ckpt-700\n",
      "INFO:tensorflow:evaluation_loop marked as finished\n",
      "ステップ：700\n",
      "{'eval_loss': 0.04941989, 'loss': 0.049409322, 'global_step': 700}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Running train on CPU\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/tmp/bert_TrEv/model.ckpt-700\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 700 into /home/ubuntu/tmp/bert_TrEv/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 800 into /home/ubuntu/tmp/bert_TrEv/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.008672821.\n",
      "INFO:tensorflow:training_loop marked as finished\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Running eval on CPU\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-31-10:02:06\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/tmp/bert_TrEv/model.ckpt-800\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-31-10:07:24\n",
      "INFO:tensorflow:Saving dict for global step 800: eval_loss = 0.04153659, global_step = 800, loss = 0.041529033\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 800: /home/ubuntu/tmp/bert_TrEv/model.ckpt-800\n",
      "INFO:tensorflow:evaluation_loop marked as finished\n",
      "ステップ：800\n",
      "{'eval_loss': 0.04153659, 'loss': 0.041529033, 'global_step': 800}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Running train on CPU\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/tmp/bert_TrEv/model.ckpt-800\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 800 into /home/ubuntu/tmp/bert_TrEv/model.ckpt.\n",
      "INFO:tensorflow:training_loop marked as finished\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-32a50d8c2038>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcurrent_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mcurrent_step\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mnum_train_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_input_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0meval_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_input_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcurrent_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_step\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msteps_per_eval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m   2401\u001b[0m       return super(TPUEstimator, self).train(\n\u001b[1;32m   2402\u001b[0m           \u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2403\u001b[0;31m           \u001b[0msaving_listeners\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2404\u001b[0m       )\n\u001b[1;32m   2405\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m       \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1205\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1207\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1209\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1239\u001b[0m       return self._train_with_estimator_spec(estimator_spec, worker_hooks,\n\u001b[1;32m   1240\u001b[0m                                              \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1241\u001b[0;31m                                              saving_listeners)\n\u001b[0m\u001b[1;32m   1242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_with_estimator_spec\u001b[0;34m(self, estimator_spec, worker_hooks, hooks, global_step_tensor, saving_listeners)\u001b[0m\n\u001b[1;32m   1469\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1470\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1471\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1472\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    669\u001b[0m                           \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m                           \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m                           run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1154\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m                               \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1156\u001b[0;31m                               run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1157\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m         logging.info('An error was raised. This may be due to a preemption in '\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1238\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1240\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1241\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m       \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m                                   \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m                                   \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1312\u001b[0;31m                                   run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1076\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1077\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_with_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "current_step = 0\n",
    "while current_step <= num_train_steps:\n",
    "    estimator.train(input_fn=train_input_fn,steps=steps_per_eval)\n",
    "    eval_result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)\n",
    "    current_step = current_step + steps_per_eval\n",
    "    print(\"ステップ：{}\".format(current_step))\n",
    "    print(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_examples = processor.get_test_examples(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 3191\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: test-0\n",
      "INFO:tensorflow:tokens: [CLS] i purchased this tablet out of enthusiasm for it ' s built in beats audio . while it does have beats audio built in , it is a very limited feature . the overall movement through the tablet is very smooth . however , i noticed early on that what should be beautiful hd graphics were grain ##y and down right ugly . i usually get nice photos on my facebook wall . on my hp laptop , and my samsung tablet the photos look beautiful , but on the slate 7 the pictures are terrible . that was my first concern . second concern , the slate 7 didn ' t come with a built in video player . the primary reason i started using [SEP]\n",
      "INFO:tensorflow:input_ids: 101 1045 4156 2023 13855 2041 1997 12024 2005 2009 1005 1055 2328 1999 10299 5746 1012 2096 2009 2515 2031 10299 5746 2328 1999 1010 2009 2003 1037 2200 3132 3444 1012 1996 3452 2929 2083 1996 13855 2003 2200 5744 1012 2174 1010 1045 4384 2220 2006 2008 2054 2323 2022 3376 10751 8389 2020 8982 2100 1998 2091 2157 9200 1012 1045 2788 2131 3835 7760 2006 2026 9130 2813 1012 2006 2026 6522 12191 1010 1998 2026 19102 13855 1996 7760 2298 3376 1010 2021 2006 1996 12796 1021 1996 4620 2024 6659 1012 2008 2001 2026 2034 5142 1012 2117 5142 1010 1996 12796 1021 2134 1005 1056 2272 2007 1037 2328 1999 2678 2447 1012 1996 3078 3114 1045 2318 2478 102\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: test-1\n",
      "INFO:tensorflow:tokens: [CLS] i never really care for android devices . i ' m a hardcore microsoft user and have used 3rd party cheap android tablet that never ever meet my expectation . i receive this hp slate 7 inches as a gift . i checked online and its a relatively cheap tablet and i wasn ' t expecting much from it . i started using it and i was caught by it . . . . it is pretty fast and accurate . the storage is expand ##able with an additional 32 ##gb and 1 . 6 ##gh ##z dual core processor make everything pretty smooth . . . . the tablet feel like a strong device that will not fall apart easily , the cameras are not [SEP]\n",
      "INFO:tensorflow:input_ids: 101 1045 2196 2428 2729 2005 11924 5733 1012 1045 1005 1049 1037 13076 7513 5310 1998 2031 2109 3822 2283 10036 11924 13855 2008 2196 2412 3113 2026 17626 1012 1045 4374 2023 6522 12796 1021 5282 2004 1037 5592 1012 1045 7039 3784 1998 2049 1037 4659 10036 13855 1998 1045 2347 1005 1056 8074 2172 2013 2009 1012 1045 2318 2478 2009 1998 1045 2001 3236 2011 2009 1012 1012 1012 1012 2009 2003 3492 3435 1998 8321 1012 1996 5527 2003 7818 3085 2007 2019 3176 3590 18259 1998 1015 1012 1020 5603 2480 7037 4563 13151 2191 2673 3492 5744 1012 1012 1012 1012 1996 13855 2514 2066 1037 2844 5080 2008 2097 2025 2991 4237 4089 1010 1996 8629 2024 2025 102\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: test-2\n",
      "INFO:tensorflow:tokens: [CLS] i had originally purchased the go ##pole reach , returned it and bought this this po ##v pole . so i can give a comparison between the two . here ' s a quick review of both : go ##pole reach : [ . . . ] - comparable in price $ 54 . 99 for go ##pole vs $ 49 . 99 for po ##v pole - 25 % heavier than the po ##v pole . 8 oz vs . 6 . 4 oz . - longer final compressed length , will not fit my back - pack . and my back - pack is already larger than most . - on my unit , there was a problem with the 3rd section and the [SEP]\n",
      "INFO:tensorflow:input_ids: 101 1045 2018 2761 4156 1996 2175 15049 3362 1010 2513 2009 1998 4149 2023 2023 13433 2615 6536 1012 2061 1045 2064 2507 1037 7831 2090 1996 2048 1012 2182 1005 1055 1037 4248 3319 1997 2119 1024 2175 15049 3362 1024 1031 1012 1012 1012 1033 1011 12435 1999 3976 1002 5139 1012 5585 2005 2175 15049 5443 1002 4749 1012 5585 2005 13433 2615 6536 1011 2423 1003 11907 2084 1996 13433 2615 6536 1012 1022 11472 5443 1012 1020 1012 1018 11472 1012 1011 2936 2345 16620 3091 1010 2097 2025 4906 2026 2067 1011 5308 1012 1998 2026 2067 1011 5308 2003 2525 3469 2084 2087 1012 1011 2006 2026 3131 1010 2045 2001 1037 3291 2007 1996 3822 2930 1998 1996 102\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: test-3\n",
      "INFO:tensorflow:tokens: [CLS] i picked up a pair of these for even a few dollars cheaper than amazon sells them , and to be honest , expected to be bringing them back . i didn ' t think something so inexpensive would actually work . i ' ve spent a lot of money on sen ##nh ##eis ##er wireless head ##phones over the years , and while they sound & work great , a design flaw causes them to end up breaking , becoming a real has ##sle to try and keep on your head , and then eventually die . these fit nice and s ##nu ##g , are comfortable , and to be honest , look like they ' ll last longer than the sen ##nh ##eis [SEP]\n",
      "INFO:tensorflow:input_ids: 101 1045 3856 2039 1037 3940 1997 2122 2005 2130 1037 2261 6363 16269 2084 9733 15187 2068 1010 1998 2000 2022 7481 1010 3517 2000 2022 5026 2068 2067 1012 1045 2134 1005 1056 2228 2242 2061 23766 2052 2941 2147 1012 1045 1005 2310 2985 1037 2843 1997 2769 2006 12411 25311 17580 2121 9949 2132 19093 2058 1996 2086 1010 1998 2096 2027 2614 1004 2147 2307 1010 1037 2640 28450 5320 2068 2000 2203 2039 4911 1010 3352 1037 2613 2038 25016 2000 3046 1998 2562 2006 2115 2132 1010 1998 2059 2776 3280 1012 2122 4906 3835 1998 1055 11231 2290 1010 2024 6625 1010 1998 2000 2022 7481 1010 2298 2066 2027 1005 2222 2197 2936 2084 1996 12411 25311 17580 102\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: test-4\n",
      "INFO:tensorflow:tokens: [CLS] it might be the winner . i have listened to a number of speakers and simply i am always kept wanting more . and by more , i mean real sound that is natural and able to fill a room with ease . for the $ 100 category - - this is by far the best thing i have listened to . although not perfect , it is really good . what most sound like is an old realistic handheld radio that now costs $ 200 . . . . and that is truly how i feel they sound . there is a lot of h ##ype with bose , ultimate ears , jam ##box , td ##k , j ##bl - - - you name [SEP]\n",
      "INFO:tensorflow:input_ids: 101 2009 2453 2022 1996 3453 1012 1045 2031 7791 2000 1037 2193 1997 7492 1998 3432 1045 2572 2467 2921 5782 2062 1012 1998 2011 2062 1010 1045 2812 2613 2614 2008 2003 3019 1998 2583 2000 6039 1037 2282 2007 7496 1012 2005 1996 1002 2531 4696 1011 1011 2023 2003 2011 2521 1996 2190 2518 1045 2031 7791 2000 1012 2348 2025 3819 1010 2009 2003 2428 2204 1012 2054 2087 2614 2066 2003 2019 2214 12689 27291 2557 2008 2085 5366 1002 3263 1012 1012 1012 1012 1998 2008 2003 5621 2129 1045 2514 2027 2614 1012 2045 2003 1037 2843 1997 1044 18863 2007 21299 1010 7209 5551 1010 9389 8758 1010 14595 2243 1010 1046 16558 1011 1011 1011 2017 2171 102\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    }
   ],
   "source": [
    "predict_features = run_regressor.convert_examples_to_features(predict_examples,max_seq_length=max_seq_length,tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_input_fn = input_fn_builder(predict_features,max_seq_length,is_training=False,drop_remainder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = estimator.predict(input_fn=predict_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Running infer on CPU\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/tmp/bert_TrEv/model.ckpt-500\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:prediction_loop marked as finished\n",
      "INFO:tensorflow:prediction_loop marked as finished\n"
     ]
    }
   ],
   "source": [
    "prediction = np.array(list(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array([i.label for i in predict_examples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f20c4418828>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(labels,prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error,r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04335372527463028\n",
      "0.2801617327273611\n"
     ]
    }
   ],
   "source": [
    "print(mean_squared_error(y_pred=prediction,y_true=labels))\n",
    "\n",
    "print(r2_score(y_pred=prediction,y_true=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   9.,   17.,   20.,   59.,   73.,  127.,  231.,  591., 1460.,\n",
       "         604.]),\n",
       " array([0.11088897, 0.19888072, 0.28687247, 0.37486422, 0.46285597,\n",
       "        0.55084772, 0.63883947, 0.72683121, 0.81482296, 0.90281471,\n",
       "        0.99080646]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(labels,alpha=0.5)\n",
    "plt.hist(prediction,alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('/home/ubuntu/glue_data/ARD/test.tsv',delimiter='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
